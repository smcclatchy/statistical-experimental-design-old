---
title: "Experimental Design Principles"
teaching: 0
exercises: 0
questions:
- "What are the core principles of experimental design?"
objectives:
- "The way in which a design applies treatments to experimental units and measures the responses will determine 1) what questions can be answered and 2) with what precision relationships can be described."
- "The core principles guiding the way are 1) replication, 2) randomization and 3) blocking."
keypoints:
- "Replication, randomization and blocking determine the validity and usefulness of an experiment."
source: Rmd
---

```{r, include = FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("03-")
```

Variability is natural in the real world. A medication given to a group of 
patients will affect each of them differently. A specific diet given to a cage
of mice will affect each mouse differently. To figure out whether a difference
in responses is real or inherently random, *replication* applies the same 
treatment to multiple experimental units. Ideally if something is measured many 
times, each measurement will give exactly the same result and will represent 
the true value. This ideal doesn't exist in the real world. For example, the 
mass of one kilogram is defined by the International Prototype Kilogram, a 
cylinder composed of platinum and iridium. Copies of this prototype kilogram 
(replicates) are distributed worldwide and are regularly compared to the 
original to ensure the standard mass of one kilogram. None of the copies of the prototype measure precisely the same despite careful storage and handling. The 
reasons for this variation in measurements is not known.

The variability of the responses within a set of replicates provides a measure 
against which we can compare differences among different treatments. This 
variability is known as *experimental error*. This does not mean that something 
was done wrongly! It's a phrase describing the variability in the responses. 
Random variation is also known as *random error* or *noise*.  It reflects 
imprecision, but not inaccuracy. Larger sample sizes reduce this imprecision.

In addition to random (experimental) error, also known as noise, there are two
other sources of variability in experiments. *Systematic error* or bias, occurs 
when there are deviations in measurements or observations that are consistently 
in one particular direction, either overestimating or underestimating the true 
value. As an example, a scale might be calibrated so that mass measurements 
are consistently too high or too low. Unlike random error, systematic error is
consistent in one direction, is predictable and follows a pattern. Larger sample 
sizes donâ€™t correct for systematic bias; equipment or measurement calibration 
does. *Technical replicates* define this systematic bias by running the same 
sample through the machine or measurement protocol multiple times to 
characterize the variation caused by equipment or protocols.

A *biological replicate* measures different biological samples in parallel to 
estimate the variation caused by the unique biology of the samples. The sample 
or group of samples are derived from the same biological source, such as cells, 
tissues, organisms, or individuals. Biological replicates assess the variability 
and reproducibility of experimental results. For example, if a study examines 
the effect of a drug on cell growth, biological replicates would involve 
multiple sets of cells from the same cell line to test the drug's effects. This 
helps to ensure that any observed changes are due to the drug itself rather than variations in the biological material being used. 

The greater the number of replications, the greater the precision (the closeness 
of two or more measurements to each other).  Having a large enough sample size 
to ensure high precision is necessary to ensure reproducible results.    

> ## Exercise 1: Which kind of error?
> A study used to determine the effect of a drug on weight loss 
> could have the following sources of experimental error. 
> Classify the following sources as either biological, 
> systematic, or random error.  
> 1). A scale is broken and provides inconsistent readings.  
> 2). A scale is calibrated wrongly and consistently measures mice 1 gram heavier.   
> 3). A mouse has an unusually high weight compared to its experimental group (i.e., it is an outlier).  
> 4). Strong atmospheric low pressure and accompanying storms affect instrument readings, animal behavior, and indoor relative humidity.  
>
> >
> > ## Solution to Exercise 1
> > 
> > 1). random, because the scale is broken and provides any kind of random reading it comes up with (inconsistent reading)  
> > 2). systematic  
> > 3). biological  
> > 4). random or systematic; you argue which and explain why
> > 
> {: .solution}
{: .challenge}

These three sources of error can be mitigated by good experimental design. 
Systematic and biological error can be mitigated through adequate numbers of 
technical and biological replicates, respectively. Random error can also be 
mitigated by experimental design, however, replicates are not effective. By 
definition random error is unpredictable or unknowable. For example, an 
atmospheric low pressure system or a strong storm could affect equipment 
measurements, animal behavior, and indoor relative humidity, which introduces 
random error. We could assume that all random error will balance itself out, and 
that all samples will be equally subject to random error. A more precise way to 
mitigate random error is through blocking. 

{% include links.md %}