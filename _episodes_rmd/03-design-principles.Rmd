---
title: "Experimental Design Principles"
teaching: 0
exercises: 0
questions:
- "What are the core principles of experimental design?"
objectives:
- "The way in which a design applies treatments to experimental units and measures the responses will determine 1) what questions can be answered and 2) with what precision relationships can be described."
- "The core principles guiding the way are 1) replication, 2) randomization and 3) blocking."
keypoints:
- "Replication, randomization and blocking determine the validity and usefulness of an experiment."
source: Rmd
---

```{r, include = FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("03-")
```

Variability is natural in the real world. A medication given to a group of 
patients will affect each of them differently. A specific diet given to a cage
of mice will affect each mouse differently. Ideally if something is measured 
many times, each measurement will give exactly the same result and will 
represent the true value. This ideal doesn't exist in the real world. For 
example, the mass of one kilogram is defined by the [https://en.wikipedia.org/wiki/International_Prototype_of_the_Kilogram](International Prototype 
Kilogram), a cylinder composed of platinum and iridium. 
![International Prototype of the Kilogram](../fig/International_prototype_of_the_kilogram_aka_Le_Grand_K)

Copies of this prototype kilogram (replicates) are distributed worldwide so each 
country hosting a replica has its own national standard kilogram. None of the 
replicas measure precisely the same despite careful storage and handling. 
The reasons for this variation in measurements are not known. A kilogram in 
Austria differs from a kilogram in Australia, which differs from that in Brazil,
Kazakhstan, Pakistan, Switzerland or the U.S. What we assume is an absolute 
measure of mass shows real-world natural variability.

## Replication
To figure out whether a difference in responses is real or inherently random, 
*replication* applies the same treatment to multiple experimental units. The
variability of the responses within a set of replicates provides a measure 
against which we can compare differences among different treatments. This 
variability is known as *experimental error*. This does not mean that something 
was done wrongly! It's a phrase describing the variability in the responses. 
Random variation is also known as *random error* or *noise*.  It reflects 
imprecision, but not inaccuracy. Larger sample sizes reduce this imprecision.

In addition to random (experimental) error, also known as noise, there are two
other sources of variability in experiments. *Systematic error* or bias, occurs 
when there are deviations in measurements or observations that are consistently 
in one particular direction, either overestimating or underestimating the true 
value. As an example, a scale might be calibrated so that mass measurements 
are consistently too high or too low. Unlike random error, systematic error is
consistent in one direction, is predictable and follows a pattern. Larger sample 
sizes don’t correct for systematic bias; equipment or measurement calibration 
does. *Technical replicates* define this systematic bias by running the same 
sample through the machine or measurement protocol multiple times to 
characterize the variation caused by equipment or protocols.

A *biological replicate* measures different biological samples in parallel to 
estimate the variation caused by the unique biology of the samples. The sample 
or group of samples are derived from the same biological source, such as cells, 
tissues, organisms, or individuals. Biological replicates assess the variability 
and reproducibility of experimental results. For example, if a study examines 
the effect of a drug on cell growth, biological replicates would involve 
multiple sets of cells from the same cell line to test the drug's effects. This 
helps to ensure that any observed changes are due to the drug itself rather than variations in the biological material being used. 

The greater the number of replications, the greater the precision (the closeness 
of two or more measurements to each other).  Having a large enough sample size 
to ensure high precision is necessary to ensure reproducible results.    

> ## Exercise 1: Which kind of error?
> A study used to determine the effect of a drug on weight loss 
> could have the following sources of experimental error. 
> Classify the following sources as either biological, 
> systematic, or random error.  
> 1). A scale is broken and provides inconsistent readings.  
> 2). A scale is calibrated wrongly and consistently measures mice 1 gram heavier.   
> 3). A mouse has an unusually high weight compared to its experimental group (i.e., it is an outlier).  
> 4). Strong atmospheric low pressure and accompanying storms affect instrument readings, animal behavior, and indoor relative humidity.  
>
> >
> > ## Solution to Exercise 1
> > 
> > 1). random, because the scale is broken and provides any kind of random reading it comes up with (inconsistent reading)  
> > 2). systematic  
> > 3). biological  
> > 4). random or systematic; you argue which and explain why
> > 
> {: .solution}
{: .challenge}

These three sources of error can be mitigated by good experimental design. 
Systematic and biological error can be mitigated through adequate numbers of 
technical and biological replicates, respectively. Random error can also be 
mitigated by experimental design, however, replicates are not effective. By 
definition random error is unpredictable or unknowable. For example, an 
atmospheric low pressure system or a strong storm could affect equipment 
measurements, animal behavior, and indoor relative humidity, which introduces 
random error. We could assume that all random error will balance itself out, and 
that all samples will be equally subject to random error. A more precise way to 
mitigate random error is through blocking. 

## Randomization
> ## Exercise 2: The efficient technician
> Your technician colleague finds a way to simplify and expedite an experiment.
> The experiment applies four different wheel-running treatments to twenty 
> different mice over the course of five days. Four mice are treated 
> individually each day for two hours each with a random selection of the four
> treatments. Your clever colleague decides that a simplified protocol would 
> work just as well and save time. Run treatment 1 five times on day 1, 
> treatment 2 five times on day 2, and so on. Some overtime would be required 
> each day but the experiment would be completed in only four days, and then 
> they can take Friday off! 
> Does this adjustment make sense to you?    
> Can you foresee any problems with the experimental results?   
>
> >
> > ## Solution to Exercise 2
> > 
> > Since each treatment is run on only one day, the day effectively becomes
> > the experimental unit (explain this). Each experimental unit (day) has five
> > samples (mice), but only one replication of each treatment. There is no 
> > valid way to compare treatments as a result. There is no way to separate
> > the treatment effect from the day-to-day differences in environment, 
> > equipment setup, personnel, and other extraneous variables.
> > 
> {: .solution}
{: .challenge}

Why should treatments be randomly assigned to experimental units? Randomization
minimizes bias and moderates experimental error (a.k.a. noise). A hat full of 
numbers, a random number table or a computational random number generator can be 
used to assign random numbers to experimental units so that any experimental 
unit has equal chances of being assigned to a specific treatment group. 

Here is an example of randomization using a random number generator. The study
asks how a high-fat diet affects blood pressure in mice. If the number is odd, 
the sample is assigned to the treatment group, which receives the high-fat diet.
If the random number is even, the sample is assigned to the control group (the 
group that doesn't receive the treatment, in this case, regular chow). 

```{r, random_number_generator}
exp_unit <- LETTERS
random_number <- sample(x = 100, size = 26)

# %% is the modulo operator, which returns the remainder from division
treatment <- ifelse(random_number %% 2 == 0, "chow", "high fat")
random_allocation <- data.frame(exp_unit, random_number, treatment)
random_allocation
```

This might produce unequal numbers between treatment and control groups. It 
isn’t necessary to have equal numbers, however, *sensitivity* (the true positive 
rate, or ability to detect an effect when it truly exists) is maximized when 
sample numbers are equal.

```{r, random_assignment_table1}
table(random_allocation$treatment)
```

To randomly assign samples to groups with equal numbers, you can do the 
following.

```{r, equal_numbers}

# place IDs and random numbers in data frame
equal_allocation <- data.frame(exp_unit, random_number)

# sort by random numbers (not by sample IDs)
equal_allocation <- equal_allocation[order(random_number),]

# now assign to treatment or control groups
treatment <- sort(rep(x = c("chow", "high fat"), times = 13))
equal_allocation <- cbind(equal_allocation, treatment)
row.names(equal_allocation) <- 1:26
equal_allocation
```

You can write out this treatment plan to a comma-separated values (csv) file,
then open it in Excel and use it to record your data collection or just keep
track of which samples are randomly assigned which diet.

```{r, write_out_plan}
write.csv(equal_allocation, file = "random-assign.csv", row.names = FALSE)
```

> ## Discussion
> Why not assign treatment and control groups to samples in alphabetical order?  
> Did we really need a random number generator to obtain randomized equal groups?
>
> >
> > ## Solution 
> > 
> > 1). Scenario: One technician processed samples A through M, and a different 
> > technician processed samples N through Z.  
> > 2). Another scenario: Samples A through M were processed on a Monday, and 
> > samples N through Z on a Tuesday.  
> > 3). Yet another scenario: Samples A through M were from one strain, and 
> > samples N through Z from a different strain.    
> > 4). Yet another scenario: Samples with consecutive ids were all sibling 
> > groups. For example, samples A, B and C were all siblings, and all assigned 
> > to the same treatment.  
> > All of these cases would have introduced an effect (from the technician, the 
> > day of the week, the strain, or sibling relationships) that would confound 
> > the results and lead to misinterpretation.
> > 
> {: .solution}
{: .challenge}

## Blocking
Experimental units can be grouped, or *blocked*, to increase the precision of
treatment comparisons. Imagine that you want to evaluate the effect of different 
doses of a new drug on the proliferation of four different cancer cell lines in
vitro. Divide each of the cell lines into four treatment groups, each with the 
same number of cells. Treat each group with a different dose of the drug for 
five consecutive days.

Group 1: Control (no drug)  
Group 2: Low dose (10 μM) 
Group 3: Medium dose (50 μM) 
Group 4: High dose (100 μM) 

```{r, complete_random_block}

# create dosage levels
f <- factor(c("control", "low", "medium", "high"))
# create random orderings of the treatment levels
b1t <- sample(f, 4)
b2t <- sample(f, 4)
b3t <- sample(f, 4)
b4t <- sample(f, 4)
t <- c(b1t, b2t, b3t, b4t)
block <- factor(rep(c("cellLine1", "cellLine2", "cellLine3", "cellLine4"), each = 4))
groupnum <- rep(1:4, 4)
plan <- data.frame(block = block, GroupNumber = groupnum, treatment = t)
plan

```
{% include links.md %}