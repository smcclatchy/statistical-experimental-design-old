---
title: "Essential Features of a Comparative Experiment"
teaching: 0
exercises: 0
questions:
- "How are comparative experiments structured?"
objectives:
- "Describe the common features of comparative experiments."
keypoints:
- "The raw ingredients of comparative experiments are experimental units, treatments and responses."
source: Rmd
---

```{r, include = FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("02-")
```

The [Generation 100 study](https://bmjopen.bmj.com/content/5/2/e007519) 
evaluated the effects of exercise on more than 1500 elderly Norwegians to 
determine if exercise led to a longer active and healthy life. Specifically the 
researchers investigated the relationship between exercise intensity and health 
and longevity. One group performed high-intensity interval training (10 minute 
warm-up followed by four 4-minute intervals at ∼90% of peak heart rate) twice a 
week for five years. A second group performed moderate exercise twice a week (50 
minutes of continuous exercise at ∼70% of peak heart rate). A third control 
group followed physical activity advice according to national recommendations. 
Clinical examinations and questionnaires were administered to all at the start 
and after one, three, and five years. Heart rate, blood pressure, leg and grip 
strength, cognitive function, and other health indicators were measured during 
clinical exams.

> ## Challenge 1: Raw ingredients of a comparative experiment
>
> Discuss the following questions with your partner, then share your answers
> to each question in the collaborative document.
>
> 1. What is the research question in this study? If you prefer to name a 
> hypothesis, turn the research question into a declarative statement.  
> 2. What are the treatments?  
> 3. What are the experimental units (the entities to which treatments are 
> applied)?  
> 4. What are the responses (the measurements used to determine treatment 
> effects)?  
> 5. Should participants have been allowed to choose which group 
> (high-intensity, moderate exercise, or national standard) they wanted to 
> join? Why or why not? Should the experimenters have assigned participants
> to treatment groups based on their judgment of each participant's 
> characteristics? Why or why not?
>
> > ## Solution
> > The research question asked whether exercise, specifically high-intensity
> > exercise, would affect healthspan and lifespan of elderly Norwegians. The
> > treatments were high-intensity, moderate-intensity, and national standard
> > exercise groups. The experimental units are the individuals. The responses 
> > measured were heart rate, blood pressure, strength, cognitive function and 
> > other health indicators. The main response measured was 5-year survival.
> > If participants had been allowed to choose their preferred exercise group 
> > or if experimenters had chosen the groups based on participant 
> > characteristics, extraneous variables (e.g. state of depression) could
> > be introduced into the study. When participants are randomly assigned to
> > treatment groups, these variables are spread across the groups and cancel
> > out. Furthermore, if experimenters had used their own judgment to assign
> > participants to groups, their own biases could have affected the results.
> {: .solution}
{: .challenge}


### Conducting a Comparative Experiment

Comparative experiments apply treatments to experimental units and measure the
responses, then compare the responses to those treatments with statistical
analysis. If in the Generation 100 study the experimenters had only one group
(e.g. high-intensity training), they might have achieved good results but would have no way of knowing if either of the other treatments would have achieved the same or even better results. To know whether high-intensity training is better than moderate or low-intensity training, it was necessary to run experiments in which some experimental units engaged in high-intensity training, others in moderate, and others still in low-intensity training. Only then can the 
responses to those treatments be statistically analyzed to determine treatment effects.

> ## Challenge 2: Which are the experimental units?
>
> Identify the experimental units in each experiment described below, then share
> your answers in the collaborative document.
>
> 1. Three hundred mice are individually housed in the same room. Half of them 
> are fed a high-fat diet and the other half are fed regular chow.
> 2. Three hundred mice are housed five per cage in the same room. Half of them 
> are fed a high-fat diet and the other half are fed regular chow.   
> 3. Three hundred mice are individually housed in two different rooms. Those in
> the first room are fed a high-fat diet and those in the other room are fed 
> regular chow.    
>
> > ## Solution
> > 1. The individual animal is the experimental unit.
> > 
> > 2. The cage receives the treatment and is the experimental unit.
> > 
> > 3. The room receives the treatment and is the experimental unit.
> > 
> {: .solution}
{: .challenge}

### Reducing Bias with Randomization and Blinding
Randomized studies assign experimental units to treatment groups randomly by
pulling a number out of a hat or using a computer's random number generator. The
main purpose for randomization comes later during statistical analysis, where 
we compare the data we obtained to the data distribution we might have 
obtained. Randomization provides us a way to create the distribution of data
we might have obtained and ensures that our comparisons between treatment groups
are valid. Random assignment (*allocation*) of experimental units to treatment groups prevents the subjective bias that might be introduced by an experimenter who selects, even in good faith and with good intention, which experimental 
units should get which treatment. For example, if the experimenter selected 
which people would do high-, moderate- and low-intensity training they might unconsciously bias the groups by body size or shape. This *selection bias* would influence the outcome of the experiment.

Randomization also accounts for or cancels out effects of "nuisance"
variables like the time or day of the experiment, the investigator or 
technician, equipment calibration, exposure to light or ventilation in animal rooms, or other variables that are not being studied but that do influence the responses. Randomization balances out the effects of nuisance variables between treatment groups by giving an equal probability for an experimental unit to be assigned to any treatment group.

Blinding (also known as masking) prevents the experimenter from influencing the
outcome of an experiment to suit their expectations or preferred hypothesis.
Ideally experimenters should not know which treatment the experimental units 
have received or will receive from the start to the statistical analysis stage 
of the experiment. This might require additional personnel like technicians or
other colleague to perform some tasks, and should be considered during
experimental design. If ideal circumstances can't be arranged, it should be possible to carry out at least some of the stages blind. Blinding during
allocation (assignment of experimental units to treatment groups), treatment,
data collection or data analysis can reduce experimental bias.

> ## Challenge 3: How does bias enter an experiment?
>
> Identify ways that bias enters into each experiment described below, then 
> share your answers in the collaborative document.
>
> 1. A clinician perceives increased aggression in subjects given testosterone.    
> 2. A clinician concludes that mood of each subject has improved in the 
> treatment group given a new antidepressant.    
> 3. A researcher unintentionally treats subjects differently based on their 
> treatment group by providing more food to control group animals.   
> 4. A clinician gives different nonverbal cues to patients in the treatment 
> group of a clinical trial than to the control group patients.  
>
> > ## Solution
> > 1 and 2 describe nonblind data collection reporting increased treatment
> > effects. Inflated effect sizes are a common problem with nonblinded studies.
> > In 3 and 4 the experimenter 
> > 
> {: .solution}
{: .challenge}

### Controlling Natural Variation with Blocking

Now that we've seen how to turn Fahrenheit into Celsius, it's easy to turn Celsius into Kelvin:

Randomisation within blocks  

Blocking is a method of controlling natural variation among experimental units. This splits up the experiment into smaller sub-experiments (blocks), and treatments are randomised to experimental units within each block [5,13,14]. This takes into account nuisance variables that could potentially bias the results (e.g. cage location, day or week of procedure).

Stratified randomisation uses the same principle as randomisation within blocks, only the strata tend to be traits of the animal that are likely to be associated with the response (e.g. weight class or tumour size class). This can lead to differences in the practical implementation of stratified randomisation as compared to block randomisation (e.g. there may not be equal numbers of experimental units in each weight class).

> ## Create a Function
>
> In the last lesson, we learned to **c**ombine elements into a vector using the `c` function,
> e.g. `x <- c("A", "B", "C")` creates a vector `x` with three elements.
> Furthermore, we can extend that vector again using `c`, e.g. `y <- c(x, "D")` creates a vector `y` with four elements.
> Write a function called `highlight` that takes two vectors as arguments, called
> `content` and `wrapper`, and returns a new vector that has the wrapper vector
> at the beginning and end of the content:
>
> ```{r, echo=-1}
> highlight <- function(content, wrapper) {
>    answer <- c(wrapper, content, wrapper)
>    return(answer)
> }
> best_practice <- c("Write", "programs", "for", "people", "not", "computers")
> asterisk <- "***"  # R interprets a variable with a single value as a vector
>                    # with one element.
> highlight(best_practice, asterisk)
> ```
>
> > ## Solution
> > ~~~
> > highlight <- function(content, wrapper) {
> >   answer <- c(wrapper, content, wrapper)
> >   return(answer)
> > }
> > ~~~
> > {: .language-r}
> {: .solution}
>
> If the variable `v` refers to a vector, then `v[1]` is the vector's first element and `v[length(v)]` is its last (the function `length` returns the number of elements in a vector).
> Write a function called `edges` that returns a vector made up of just the first and last elements of its input:
>
> ```{r, echo=-1}
> edges <- function(v) {
>    first <- v[1]
>    last <- v[length(v)]
>    answer <- c(first, last)
>    return(answer)
> }
> dry_principle <- c("Don't", "repeat", "yourself", "or", "others")
> edges(dry_principle)
> ```
>
> > ## Solution
> > ~~~
> > edges <- function(v) {
> >    first <- v[1]
> >    last <- v[length(v)]
> >    answer <- c(first, last)
> >    return(answer)
> > }
> > ~~~
> > {: .language-r}
> {: .solution}
{: .challenge}

> ## The Call Stack
>
> For a deeper understanding of how functions work,
> you'll need to learn how they create their own environments and call other functions.
> Function calls are managed via the call stack.
> For more details on the call stack,
> have a look at the [supplementary material]({{ page.root }}/14-supp-call-stack/).
{: .callout}

> ## Named Variables and the Scope of Variables
>
> Functions can accept arguments explicitly assigned to a variable name in
> the function call `functionName(variable = value)`, as well as arguments by
> order:
>
> ```{r}
> input_1 <- 20
> mySum <- function(input_1, input_2 = 10) {
>   output <- input_1 + input_2
>   return(output)
> }
> ```
>
> 1.  Given the above code was run, which value does `mySum(input_1 = 1, 3)` produce?
>     1. 4
>     2. 11
>     3. 23
>     4. 30
> 2.  If `mySum(3)` returns 13, why does `mySum(input_2 = 3)` return an error?
>
> > ## Solution
> > 1. The solution is `1.`.
> > 
> > 2. Read the error message: `argument "input_1" is missing, with no default`
> > means that no value for `input_1` is provided in the function call, 
> > and neither in the function's defintion. Thus, the addition in the
> > function body can not be completed.
> > 
> {: .solution}
{: .challenge}


### Testing, Error Handling, and Documenting

Once we start putting things in functions so that we can re-use them, we need to start testing that those functions are working correctly.
To see how to do this, let's write a function to center a dataset around a
particular midpoint:

```{r}
center <- function(data, midpoint) {
  new_data <- (data - mean(data)) + midpoint
  return(new_data)
}
```

We could test this on our actual data, but since we don't know what the values ought to be, it will be hard to tell if the result was correct.
Instead, let's create a vector of 0s and then center that around 3.
This will make it simple to see if our function is working as expected:

```{r, }
z <- c(0, 0, 0, 0)
z
center(z, 3)
```

That looks right, so let's try center on our real data. We'll center the inflammation data from day 4 around 0:

```{r}
dat <- read.csv(file = "data/inflammation-01.csv", header = FALSE)
centered <- center(dat[, 4], 0)
head(centered)
```

It's hard to tell from the default output whether the result is correct, but there are a few simple tests that will reassure us:

```{r}
# original mean
mean(dat[, 4])
# centered mean
mean(centered)
```

That seems right: the original mean was about `r round(mean(dat[, 4]), 2)` and the mean of the centered data is `r mean(centered)`.
We can even go further and check that the standard deviation hasn't changed:

```{r}
# original standard deviation
sd(dat[, 4])
# centered standard deviation
sd(centered)
```

Those values look the same, but we probably wouldn't notice if they were different in the sixth decimal place.
Let's do this instead:

```{r}
# difference in standard deviations before and after
sd(dat[, 4]) - sd(centered)
```

Sometimes, a very small difference can be detected due to rounding at very low decimal places.
R has a useful function for comparing two objects allowing for rounding errors, `all.equal`:

```{r}
all.equal(sd(dat[, 4]), sd(centered))
```

It's still possible that our function is wrong, but it seems unlikely enough that we should probably get back to doing our analysis.
However, there are two other important tasks to consider: 1) we should ensure our function can provide informative errors when needed, and 2) we should write some [documentation]({{ page.root }}/reference.html#documentation) for our function to remind ourselves later what it's for and how to use it.


#### Error Handling

What happens if we have missing data (NA values) in the `data` argument we provide to `center`?

```{r}
# new data object and set one value in column 4 to NA
datNA <- dat
datNA[10,4] <- NA

# returns all NA values
center(datNA[,4], 0)

```

This is likely not the behavior we want, and is caused by the `mean` function returning NA when the `na.rm=TRUE` is not provided. We may wish to not consider NA values in our `center` function. We can provide the `na.rm=TRUE` argument and solve this issue. 
```{r}
center <- function(data, midpoint) {
  new_data <- (data - mean(data, na.rm=TRUE)) + midpoint
  return(new_data)
}

center(datNA[,4], 0)
```

However, what happens if the user were to accidentally hand this function a `factor` or `character` vector?


```{r}

datNA[,1] <- as.factor(datNA[,1])
datNA[,2] <- as.character(datNA[,2])

center(datNA[,1], 0)
center(datNA[,2], 0)

```

Both of these attempts result in errors. Luckily, the errors are quite informative. In other cases, we may need to add in error handling using the `warning` and `stop` functions.

For instance, the `center` function only works on numeric vectors. Recognizing this and adding warnings and errors provides feedback to the user and makes sure the output of the function is what the user wanted. 









#### Documentation

A common way to put documentation in software is to add [comments]({{ page.root }}/reference.html#comment) like this:

```{r}
center <- function(data, midpoint) {
  # return a new vector containing the original data centered around the
  # midpoint.
  # Example: center(c(1, 2, 3), 0) => c(-1, 0, 1)
  new_data <- (data - mean(data)) + midpoint
  return(new_data)
}
```

> ## Writing Documentation
>
> Formal documentation for R functions is written in separate `.Rd` using a
> markup language similar to [LaTeX][]. You see the result of this documentation
> when you look at the help file for a given function, e.g. `?read.csv`.
> The [roxygen2][] package allows R coders to write documentation alongside
> the function code and then process it into the appropriate `.Rd` files.
> You will want to switch to this more formal method of writing documentation
> when you start writing more complicated R projects.
{: .callout}

[LaTeX]: https://www.latex-project.org/
[roxygen2]: https://cran.r-project.org/package=roxygen2/vignettes/rd.html

```{r challenge-more-advanced-function-analyze, eval=FALSE, include=FALSE}
analyze <- function(filename) {
  # Plots the average, min, and max inflammation over time.
  # Input is character string of a csv file.
  dat <- read.csv(file = filename, header = FALSE)
  avg_day_inflammation <- apply(dat, 2, mean)
  plot(avg_day_inflammation)
  max_day_inflammation <- apply(dat, 2, max)
  plot(max_day_inflammation)
  min_day_inflammation <- apply(dat, 2, min)
  plot(min_day_inflammation)
}
```

```{r challenge-more-advanced-function-rescale, include=FALSE}
rescale <- function(v) {
  # Rescales a vector, v, to lie in the range 0 to 1.
  L <- min(v)
  H <- max(v)
  result <- (v - L) / (H - L)
  return(result)
}
```

> ## Functions to Create Graphs
>
> Write a function called `analyze` that takes a filename as an argument
> and displays the three graphs produced in the [previous lesson][01] (average, min and max inflammation over time).
> `analyze("data/inflammation-01.csv")` should produce the graphs already shown,
> while `analyze("data/inflammation-02.csv")` should produce corresponding graphs for the second data set.
> Be sure to document your function with comments.
>
> > ## Solution
> > ~~~
> > analyze <- function(filename) {
> >   # Plots the average, min, and max inflammation over time.
> >   # Input is character string of a csv file.
> >   dat <- read.csv(file = filename, header = FALSE)
> >   avg_day_inflammation <- apply(dat, 2, mean)
> >   plot(avg_day_inflammation)
> >   max_day_inflammation <- apply(dat, 2, max)
> >   plot(max_day_inflammation)
> >   min_day_inflammation <- apply(dat, 2, min)
> >   plot(min_day_inflammation)
> > }
> > ~~~
> > {: .language-r}
> {: .solution}
{: .challenge}

> ## Rescaling
>
> Write a function `rescale` that takes a vector as input and returns a corresponding vector of values scaled to lie in the range 0 to 1.
> (If `L` and `H` are the lowest and highest values in the original vector, then the replacement for a value `v` should be `(v-L) / (H-L)`.)
> Be sure to document your function with comments.
>
> Test that your `rescale` function is working properly using `min`, `max`, and `plot`.
>
> > ## Solution
> > ~~~
> > rescale <- function(v) {
> >   # Rescales a vector, v, to lie in the range 0 to 1.
> >   L <- min(v)
> >   H <- max(v)
> >   result <- (v - L) / (H - L)
> >   return(result)
> > }
> > ~~~
> > {: .language-r}
> {: .solution}
{: .challenge}

[01]: {{ page.root }}/01-starting-with-data/

```{r rescale-test, include=FALSE}
answer <- rescale(dat[, 4])
min(answer)
max(answer)
plot(answer)
plot(dat[, 4], answer)  # This hasn't been introduced yet, but it may be
                        # useful to show when explaining the answer.
```

### Defining Defaults

We have passed arguments to functions in two ways: directly, as in `dim(dat)`, and by name, as in `read.csv(file = "data/inflammation-01.csv", header = FALSE)`.
In fact, we can pass the arguments to `read.csv` without naming them:

```{r}
dat <- read.csv("data/inflammation-01.csv", FALSE)
```

However, the position of the arguments matters if they are not named.

```{r, error = TRUE}
dat <- read.csv(header = FALSE, file = "data/inflammation-01.csv")
dat <- read.csv(FALSE, "data/inflammation-01.csv")
```

To understand what's going on, and make our own functions easier to use, let's re-define our `center` function like this:

```{r}
center <- function(data, midpoint = 0) {
  # return a new vector containing the original data centered around the
  # midpoint (0 by default).
  # Example: center(c(1, 2, 3), 0) => c(-1, 0, 1)
  new_data <- (data - mean(data)) + midpoint
  return(new_data)
}
```

The key change is that the second argument is now written `midpoint = 0` instead of just `midpoint`.
If we call the function with two arguments, it works as it did before:

```{r}
test_data <- c(0, 0, 0, 0)
center(test_data, 3)
```

But we can also now call `center()` with just one argument, in which case `midpoint` is automatically assigned the default value of `0`:

```{r}
more_data <- 5 + test_data
more_data
center(more_data)
```

This is handy: if we usually want a function to work one way, but occasionally need it to do something else, we can allow people to pass an argument when they need to but provide a default to make the normal case easier.

The example below shows how R matches values to arguments

```{r}
display <- function(a = 1, b = 2, c = 3) {
  result <- c(a, b, c)
  names(result) <- c("a", "b", "c")  # This names each element of the vector
  return(result)
}

# no arguments
display()
# one argument
display(55)
# two arguments
display(55, 66)
# three arguments
display(55, 66, 77)
```

As this example shows, arguments are matched from left to right, and any that haven't been given a value explicitly get their default value.
We can override this behavior by naming the value as we pass it in:

```{r}
# only setting the value of c
display(c = 77)
```

> ## Matching Arguments
>
> To be precise, R has three ways that arguments supplied
> by you are matched to the *formal arguments* of the function definition:
>
> 1. by complete name,
> 2. by partial name (matching on initial *n* characters of the argument name), and
> 3. by position.
>
> Arguments are matched in the manner outlined above in *that order*: by
> complete name, then by partial matching of names, and finally by position.
{: .callout}

With that in hand, let's look at the help for `read.csv()`:

```{r, eval=FALSE}
?read.csv
```

There's a lot of information there, but the most important part is the first couple of lines:

```{r, eval=FALSE}
read.csv(file, header = TRUE, sep = ",", quote = "\"",
         dec = ".", fill = TRUE, comment.char = "", ...)
```

This tells us that `read.csv()` has one argument, `file`, that doesn't have a default value, and six others that do.
Now we understand why the following gives an error:

```{r, results="hide", error = TRUE}
dat <- read.csv(FALSE, "data/inflammation-01.csv")
```

It fails because `FALSE` is assigned to `file` and the filename is assigned to the argument `header`.

> ## A Function with Default Argument Values
>
> Rewrite the `rescale` function so that it scales a vector to lie between 0 and 1 by default, but will allow the caller to specify lower and upper bounds if they want.
> Compare your implementation to your neighbor's:
> Do your two implementations produce the same results when
> both are given the same input vector and parameters?
>
> > ## Solution
> > ~~~
> > rescale <- function(v, lower = 0, upper = 1) {
> >   # Rescales a vector, v, to lie in the range lower to upper.
> >   L <- min(v)
> >   H <- max(v)
> >   result <- (v - L) / (H - L) * (upper - lower) + lower
> >   return(result)
> > }
> > ~~~
> > {: .language-r}
> {: .solution}
{: .challenge}

```{r, include=FALSE}
rescale <- function(v, lower = 0, upper = 1) {
  # Rescales a vector, v, to lie in the range lower to upper.
  L <- min(v)
  H <- max(v)
  result <- (v - L) / (H - L) * (upper - lower) + lower
  return(result)
}
answer <- rescale(dat[, 4], lower = 2, upper = 5)
min(answer)
max(answer)
answer <- rescale(dat[, 4], lower = -5, upper = -2)
min(answer)
max(answer)
```

{% include links.md %}