---
title: "Statistics in Data Analysis"
teaching: 0
exercises: 0
questions:
- "How can information be extracted and communicated from experimental data?"
objectives:
- "Plotting reveals information in the data."
- "Statistical significance testing compares experimental data obtained to probability distributions of data that might also be possible."
- "A probability distribution is a mathematical function that gives the probabilities of different possible outcomes for an experiment."
keypoints:
- "Plotting and significance testing describe patterns in the data and quantify effects against random variation."
source: Rmd
---

```{r, include = FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("03-")
```

## The first step in data analysis: plot the data!
A picture is worth a thousand words, and a picture of your data could reveal
important information that can guide you forward. So first, plot the data!

```{r, simulate_gen100_data}
# simulate Generation 100 resting heart rate data for the 1,567 study 
# participants
# rnorm() generates a random normal distribution from the mean and standard
# deviation values you provide
simulated_heart_rates <- rnorm(n = 1567, mean = 80, sd = 10)
hist(simulated_heart_rates, xlab = "resting heart rate")
```
## A picture is worth a thousand words

> ## Exercise 1: What does this picture tell you about resting heart rates?  
> Are there gaps in the data?  
> Are there large clusters of similar heart rate values in the data?  
> Are there apparent outliers?  
> Do the left and right tails of the data seem to mirror each other or not?
> >
> > ## Solution to Exercise 1
> > 
> > 
> > 
> {: .solution}
{: .challenge}

Now create a boxplot of the same data.

```{r, boxplot_simulated_gen100_data}
boxplot(simulated_heart_rates)
```

> ## Exercise 2: What does this boxplot tell you about resting heart rates?  
> What does the box signify?  
> What does horizontal black line dividing the box signify?  
> Are there apparent outliers?  
> How does the boxplot relate to the histogram?
> >
> > ## Solution to Exercise 2
> > 
> > 
> > 
> {: .solution}
{: .challenge}
## Plotting


```{r, random_number_generator}
exp_unit <- LETTERS
random_number <- sample(x = 100, size = 26)

# %% is the modulo operator, which returns the remainder from division
treatment <- ifelse(random_number %% 2 == 0, "chow", "high fat")
random_allocation <- data.frame(exp_unit, random_number, treatment)
random_allocation
```



```{r, random_assignment_table1}
table(random_allocation$treatment)
```



```{r, equal_numbers}

# place IDs and random numbers in data frame
equal_allocation <- data.frame(exp_unit, random_number)

# sort by random numbers (not by sample IDs)
equal_allocation <- equal_allocation[order(random_number),]

# now assign to treatment or control groups
treatment <- sort(rep(x = c("chow", "high fat"), times = 13))
equal_allocation <- cbind(equal_allocation, treatment)
row.names(equal_allocation) <- 1:26
equal_allocation
```


```{r, write_out_plan}
write.csv(equal_allocation, file = "random-assign.csv", row.names = FALSE)
```
> ## Discussion
> Why not assign treatment and control groups to samples in alphabetical order?  
> Did we really need a random number generator to obtain randomized equal groups?
>
> >
> > ## Solution 
> > 
> > 1). Scenario: One technician processed samples A through M, and a different 
> > technician processed samples N through Z.  
> > 2). Another scenario: Samples A through M were processed on a Monday, and 
> > samples N through Z on a Tuesday.  
> > 3). Yet another scenario: Samples A through M were from one strain, and 
> > samples N through Z from a different strain.    
> > 4). Yet another scenario: Samples with consecutive ids were all sibling 
> > groups. For example, samples A, B and C were all siblings, and all assigned 
> > to the same treatment.  
> > All of these cases would have introduced an effect (from the technician, the 
> > day of the week, the strain, or sibling relationships) that would confound 
> > the results and lead to misinterpretation.
> > 
> {: .solution}
{: .challenge}

## Statistical significance testing

## Probability and distributions

## The perils of p-values

## The t-test

## Confidence intervals

## Sample sizes and power curves

## Comparing standard deviations


{% include links.md %}